"""
Clonage vocal avec Chatterbox (Resemble AI) ‚Äî v0.1.6+.
Supporte Turbo (350M params, 1-step decoder, plus rapide) et Multilingual (qualit√© max).
23 langues dont le fran√ßais. MIT License.
Optimis√© : ROCm (AMD GPU), CUDA (NVIDIA), ou CPU avec gestion m√©moire.
"""
import gc

import torch
import torchaudio as ta
from pathlib import Path

from core.hardware import get_profile


class VoiceCloner:
    """Clone une voix √† partir d'un √©chantillon audio de 3-15 secondes.

    Supporte deux modes :
    - turbo: Chatterbox-Turbo (350M params, 1-step decoder, plus rapide, moins de VRAM)
    - quality: Chatterbox Multilingual (qualit√© max, plus lent)
    """

    SUPPORTED_LANGUAGES = [
        "ar", "da", "de", "el", "en", "es", "fi", "fr", "he", "hi",
        "it", "ja", "ko", "ms", "nl", "no", "pl", "pt", "ru", "sv",
        "sw", "tr", "zh",
    ]

    def __init__(self, device: str = "auto", mode: str = "turbo"):
        """
        Args:
            device: "cuda", "cpu", ou "auto"
            mode: "turbo" (rapide, 350M params) ou "quality" (qualit√© max)
        """
        hw = get_profile()
        if device == "auto":
            # ROCm et CUDA apparaissent tous deux comme "cuda" via PyTorch
            self.device = "cuda" if hw.gpu_available else "cpu"
        else:
            self.device = device
        self.mode = mode
        self._model = None

    @property
    def model(self):
        """Lazy loading du mod√®le Chatterbox (Turbo ou Multilingual)."""
        if self._model is None:
            hw = get_profile()
            backend = hw.gpu_backend if hw.gpu_available else "cpu"

            # Lib√©rer la m√©moire GPU avant de charger un gros mod√®le
            if self.device == "cuda":
                torch.cuda.empty_cache()
                gc.collect()

            if self.mode == "turbo":
                from chatterbox.tts import ChatterboxTTS
                print(f"‚è≥ Chargement Chatterbox-Turbo ({self.device}, backend={backend})...")
                self._model = ChatterboxTTS.from_pretrained(device=self.device)
            else:
                from chatterbox.tts import ChatterboxMultilingualTTS
                print(f"‚è≥ Chargement Chatterbox Multilingual ({self.device}, backend={backend})...")
                self._model = ChatterboxMultilingualTTS.from_pretrained(device=self.device)

            # Optimisation : mode eval + d√©sactiver les gradients
            self._model.eval()

            if hw.gpu_vram_mb >= 12000 and self.device == "cuda":
                try:
                    self._model.half()
                    print("   ‚Üí Half precision (float16) activ√©")
                except Exception:
                    pass  # Certains modules ne supportent pas half

            model_name = "Turbo" if self.mode == "turbo" else "Multilingual"
            print(f"‚úÖ Mod√®le vocal {model_name} charg√© ! (VRAM: {hw.gpu_vram_mb}MB)")
        return self._model

    def generate(
        self,
        text: str,
        voice_sample_path: str,
        output_path: str,
        language: str = "fr",
        exaggeration: float = 0.5,
        cfg_weight: float = 0.5,
    ) -> str:
        """
        G√©n√®re un audio avec la voix clon√©e.

        Args:
            text: Texte √† synth√©tiser
            voice_sample_path: √âchantillon audio de r√©f√©rence (3-15s, WAV)
            output_path: Chemin de sortie (.wav)
            language: Code ISO langue (fr, en, es, de...)
            exaggeration: Expressivit√© (0.0=monotone ‚Üí 1.5=tr√®s expressif)
            cfg_weight: Fid√©lit√© texte (0.3=naturel ‚Üí 0.7=fid√®le)

        Returns:
            Chemin du fichier audio g√©n√©r√©
        """
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        print(f"üéôÔ∏è G√©n√©ration vocale [{language}] : {text[:60]}...")

        with torch.no_grad():
            if self.mode == "turbo":
                # Chatterbox-Turbo : API simplifi√©e
                wav = self.model.generate(
                    text,
                    audio_prompt_path=voice_sample_path,
                    exaggeration=exaggeration,
                    cfg_weight=cfg_weight,
                )
            else:
                # Chatterbox Multilingual : support langue explicite
                wav = self.model.generate(
                    text,
                    audio_prompt_path=voice_sample_path,
                    language_id=language,
                    exaggeration=exaggeration,
                    cfg_weight=cfg_weight,
                )

        ta.save(output_path, wav, self.model.sr)
        print(f"‚úÖ Audio : {output_path}")
        return output_path

    def unload(self) -> None:
        """Lib√®re le mod√®le de la m√©moire GPU/RAM."""
        if self._model is not None:
            del self._model
            self._model = None
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            print("üóëÔ∏è Mod√®le vocal d√©charg√©")
